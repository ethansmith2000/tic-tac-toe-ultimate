{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500/5000 - Epsilon: 0.082\n",
      "Episode 1000/5000 - Epsilon: 0.010\n",
      "Episode 1500/5000 - Epsilon: 0.010\n",
      "Episode 2000/5000 - Epsilon: 0.010\n",
      "Episode 2500/5000 - Epsilon: 0.010\n",
      "Episode 3000/5000 - Epsilon: 0.010\n",
      "Episode 3500/5000 - Epsilon: 0.010\n",
      "Episode 4000/5000 - Epsilon: 0.010\n",
      "Episode 4500/5000 - Epsilon: 0.010\n",
      "Episode 5000/5000 - Epsilon: 0.010\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "[[0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "[[ 0  1  0  0]\n",
      " [ 0  0 -1  0]\n",
      " [ 0  0  0  0]\n",
      " [ 0  0  0  0]]\n",
      "[[ 0  1  0  0]\n",
      " [ 0  0 -1  0]\n",
      " [ 0  0  0  1]\n",
      " [ 0  0  0  0]]\n",
      "[[ 0  1  0 -1]\n",
      " [ 0  0 -1  0]\n",
      " [ 0  0  0  1]\n",
      " [ 0  0  0  0]]\n",
      "[[ 0  1  0 -1]\n",
      " [ 0  0 -1  0]\n",
      " [ 0  0  0  1]\n",
      " [ 0  0  0  1]]\n",
      "[[ 0  1  0 -1]\n",
      " [ 0  0 -1  0]\n",
      " [ 0  0  0  1]\n",
      " [ 0 -1  0  1]]\n",
      "[[ 0  1  1 -1]\n",
      " [ 0  0 -1  0]\n",
      " [ 0  0  0  1]\n",
      " [ 0 -1  0  1]]\n",
      "[[ 0  1  1 -1]\n",
      " [ 0  0 -1 -1]\n",
      " [ 0  0  0  1]\n",
      " [ 0 -1  0  1]]\n",
      "[[ 0  1  1 -1]\n",
      " [ 0  1 -1 -1]\n",
      " [ 0  0  0  1]\n",
      " [ 0 -1  0  1]]\n",
      "[[ 0  1  1 -1]\n",
      " [ 0  1 -1 -1]\n",
      " [-1  0  0  1]\n",
      " [ 0 -1  0  1]]\n",
      "[[ 1  1  1 -1]\n",
      " [ 0  1 -1 -1]\n",
      " [-1  0  0  1]\n",
      " [ 0 -1  0  1]]\n",
      "[[ 1  1  1 -1]\n",
      " [ 0  1 -1 -1]\n",
      " [-1  0 -1  1]\n",
      " [ 0 -1  0  1]]\n",
      "[[ 1  1  1 -1]\n",
      " [ 1  1 -1 -1]\n",
      " [-1  0 -1  1]\n",
      " [ 0 -1  0  1]]\n",
      "[[ 1  1  1 -1]\n",
      " [ 1  1 -1 -1]\n",
      " [-1 -1 -1  1]\n",
      " [ 0 -1  0  1]]\n",
      "[[ 1  1  1 -1]\n",
      " [ 1  1 -1 -1]\n",
      " [-1 -1 -1  1]\n",
      " [ 1 -1  0  1]]\n",
      "[[ 1  1  1 -1]\n",
      " [ 1  1 -1 -1]\n",
      " [-1 -1 -1  1]\n",
      " [ 1 -1 -1  1]]\n",
      "Reward: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self, board_size=3, win_length=3):\n",
    "        self.board_size = board_size\n",
    "        self.win_length = win_length  # number of markers in a row needed to win\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.board_size, self.board_size), dtype=int)\n",
    "        self.current_player = 1  # 1 for agent, -1 for opponent\n",
    "        self.done = False\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        # Represent state as a tuple of rows so that it is hashable.\n",
    "        return tuple(map(tuple, self.board))\n",
    "\n",
    "    def available_actions(self):\n",
    "        # Return list of indices (i, j) for empty cells.\n",
    "        return list(zip(*np.where(self.board == 0)))\n",
    "\n",
    "    def step(self, action):\n",
    "        i, j = action\n",
    "        if self.board[i, j] != 0:\n",
    "            # Illegal move, can penalize heavily\n",
    "            return self.get_state(), -10, True  # end the game on illegal move\n",
    "\n",
    "        # Make the move\n",
    "        self.board[i, j] = self.current_player\n",
    "        \n",
    "        # Check for win or draw\n",
    "        if self.check_win(i, j):\n",
    "            reward = 1 if self.current_player == 1 else -1\n",
    "            self.done = True\n",
    "        elif len(self.available_actions()) == 0:\n",
    "            reward = 0  # draw\n",
    "            self.done = True\n",
    "        else:\n",
    "            reward = 0\n",
    "            self.current_player *= -1  # switch player\n",
    "\n",
    "        return self.get_state(), reward, self.done\n",
    "\n",
    "    def check_win(self, row, col):\n",
    "        # Check all directions: horizontal, vertical, and two diagonals.\n",
    "        directions = [(0,1), (1,0), (1,1), (1,-1)]\n",
    "        for dr, dc in directions:\n",
    "            count = 1  # count the current cell\n",
    "            # Check one direction\n",
    "            r, c = row + dr, col + dc\n",
    "            while 0 <= r < self.board_size and 0 <= c < self.board_size and self.board[r, c] == self.current_player:\n",
    "                count += 1\n",
    "                r += dr\n",
    "                c += dc\n",
    "            # Check the opposite direction\n",
    "            r, c = row - dr, col - dc\n",
    "            while 0 <= r < self.board_size and 0 <= c < self.board_size and self.board[r, c] == self.current_player:\n",
    "                count += 1\n",
    "                r -= dr\n",
    "                c -= dc\n",
    "            if count >= self.win_length:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Q-learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):\n",
    "        self.Q = defaultdict(float)  # Q-table, key: (state, action), value: Q-value\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "    def choose_action(self, state, available_actions):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(available_actions)\n",
    "        q_values = [self.Q[(state, action)] for action in available_actions]\n",
    "        max_q = max(q_values)\n",
    "        # If multiple actions have the same Q-value, choose one at random.\n",
    "        best_actions = [action for action, q in zip(available_actions, q_values) if q == max_q]\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done, available_actions_next):\n",
    "        current_q = self.Q[(state, action)]\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            next_q = max([self.Q[(next_state, a)] for a in available_actions_next], default=0)\n",
    "            target = reward + self.gamma * next_q\n",
    "        self.Q[(state, action)] = current_q + self.alpha * (target - current_q)\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.min_epsilon)\n",
    "\n",
    "# Training loop\n",
    "def train(agent, env, episodes=5000):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            available_actions = env.available_actions()\n",
    "            action = agent.choose_action(state, available_actions)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.learn(state, action, reward, next_state, done, env.available_actions())\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        agent.update_epsilon()\n",
    "        if (episode+1) % 500 == 0:\n",
    "            print(f\"Episode {episode+1}/{episodes} - Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    board_size = 4  # You can change this to any size you like.\n",
    "    win_length = 4  # Change accordingly if you want a different win condition.\n",
    "    env = TicTacToe(board_size=board_size, win_length=win_length)\n",
    "    agent = QLearningAgent()\n",
    "\n",
    "    train(agent, env, episodes=5000)\n",
    "\n",
    "    # Test a single game after training\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        print(np.array(state))\n",
    "        available_actions = env.available_actions()\n",
    "        action = agent.choose_action(state, available_actions)\n",
    "        state, reward, done = env.step(action)\n",
    "        if done:\n",
    "            print(np.array(state))\n",
    "            print(\"Reward:\", reward)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
